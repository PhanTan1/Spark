{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fac26a6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+--------------------+------------+-----------+--------------------+---------------+-----------+------+----------------+------+--------------------+-------------------+\n",
      "|film_id|           title|         description|release_year|language_id|original_language_id|rental_duration|rental_rate|length|replacement_cost|rating|    special_features|        last_update|\n",
      "+-------+----------------+--------------------+------------+-----------+--------------------+---------------+-----------+------+----------------+------+--------------------+-------------------+\n",
      "|      1|ACADEMY DINOSAUR|An Epic Drama of ...|        2006|          1|                null|              6|       0.99|    86|           20.99|    PG|Deleted Scenes,Be...|2006-02-15 05:03:42|\n",
      "|      2|  ACE GOLDFINGER|A Astounding Epis...|        2006|          1|                null|              3|       4.99|    48|           12.99|     G|Trailers,Deleted ...|2006-02-15 05:03:42|\n",
      "|      3|ADAPTATION HOLES|A Astounding Refl...|        2006|          1|                null|              7|       2.99|    50|           18.99| NC-17|Trailers,Deleted ...|2006-02-15 05:03:42|\n",
      "|      4|AFFAIR PREJUDICE|A Fanciful Docume...|        2006|          1|                null|              5|       2.99|   117|           26.99|     G|Commentaries,Behi...|2006-02-15 05:03:42|\n",
      "|      5|     AFRICAN EGG|A Fast-Paced Docu...|        2006|          1|                null|              6|       2.99|   130|           22.99|     G|      Deleted Scenes|2006-02-15 05:03:42|\n",
      "+-------+----------------+--------------------+------------+-----------+--------------------+---------------+-----------+------+----------------+------+--------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkDemo\").getOrCreate()\n",
    "\n",
    "df_test = spark.read.csv('data/film.csv', header=True, sep=',')\n",
    "df_test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd8a6ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/06 13:10:55 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+----------------+--------------------+------------+-----------+--------------------+------------------+------------------+-----------------+------------------+------+--------------------+-------------------+\n",
      "|summary|          film_id|           title|         description|release_year|language_id|original_language_id|   rental_duration|       rental_rate|           length|  replacement_cost|rating|    special_features|        last_update|\n",
      "+-------+-----------------+----------------+--------------------+------------+-----------+--------------------+------------------+------------------+-----------------+------------------+------+--------------------+-------------------+\n",
      "|  count|             1000|            1000|                1000|        1000|       1000|                   0|              1000|              1000|             1000|              1000|  1000|                1000|               1000|\n",
      "|   mean|            500.5|            null|                null|      2006.0|        1.0|                null|             4.985| 2.979999999999938|          115.272|19.984000000000144|  null|                null|               null|\n",
      "| stddev|288.8194360957494|            null|                null|         0.0|        0.0|                null|1.4116542663725304|1.6463932126350052|40.42633181855983| 6.050832717616358|  null|                null|               null|\n",
      "|    min|                1|ACADEMY DINOSAUR|A Action-Packed C...|        2006|          1|                null|                 3|              0.99|              100|             10.99|     G|   Behind the Scenes|2006-02-15 05:03:42|\n",
      "|    max|              999|       ZORRO ARK|An Epic Yarn of a...|        2006|          1|                null|                 7|              4.99|               99|              9.99|     R|Trailers,Deleted ...|2006-02-15 05:03:42|\n",
      "+-------+-----------------+----------------+--------------------+------------+-----------+--------------------+------------------+------------------+-----------------+------------------+------+--------------------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_test.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26f32337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[film_id: string, title: string, description: string, release_year: string, language_id: string, original_language_id: string, rental_duration: string, rental_rate: string, length: string, replacement_cost: string, rating: string, special_features: string, last_update: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display\n",
    "display(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "134d0201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+--------------------+------------+-----------+--------------------+---------------+-----------+------+----------------+------+--------------------+-------------------+--------------------+--------------------+\n",
      "|film_id|           title|         description|release_year|language_id|original_language_id|rental_duration|rental_rate|length|replacement_cost|rating|    special_features|        last_update|            Fullname|      Title and year|\n",
      "+-------+----------------+--------------------+------------+-----------+--------------------+---------------+-----------+------+----------------+------+--------------------+-------------------+--------------------+--------------------+\n",
      "|      1|ACADEMY DINOSAUR|An Epic Drama of ...|        2006|          1|                null|              6|       0.99|    86|           20.99|    PG|Deleted Scenes,Be...|2006-02-15 05:03:42|ACADEMY DINOSAUR ...|ACADEMY DINOSAUR ...|\n",
      "|      2|  ACE GOLDFINGER|A Astounding Epis...|        2006|          1|                null|              3|       4.99|    48|           12.99|     G|Trailers,Deleted ...|2006-02-15 05:03:42| ACE GOLDFINGER 2006| ACE GOLDFINGER 2006|\n",
      "|      3|ADAPTATION HOLES|A Astounding Refl...|        2006|          1|                null|              7|       2.99|    50|           18.99| NC-17|Trailers,Deleted ...|2006-02-15 05:03:42|ADAPTATION HOLES ...|ADAPTATION HOLES ...|\n",
      "|      4|AFFAIR PREJUDICE|A Fanciful Docume...|        2006|          1|                null|              5|       2.99|   117|           26.99|     G|Commentaries,Behi...|2006-02-15 05:03:42|AFFAIR PREJUDICE ...|AFFAIR PREJUDICE ...|\n",
      "|      5|     AFRICAN EGG|A Fast-Paced Docu...|        2006|          1|                null|              6|       2.99|   130|           22.99|     G|      Deleted Scenes|2006-02-15 05:03:42|    AFRICAN EGG 2006|    AFRICAN EGG 2006|\n",
      "+-------+----------------+--------------------+------------+-----------+--------------------+---------------+-----------+------+----------------+------+--------------------+-------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_test = df_test.withColumn(\"Title and year\",\n",
    "    F.concat(df_test['title'], F.lit(' '), df_test['release_year'])\n",
    ")\n",
    "df_test.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "938a8731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+----------+----+---------+\n",
      "|_corrupt_record|age |city      |id  |name     |\n",
      "+---------------+----+----------+----+---------+\n",
      "|[              |null|null      |null|null     |\n",
      "|null           |30  |Paris     |1   |Alice    |\n",
      "|null           |25  |Lyon      |2   |Bob      |\n",
      "|null           |35  |Marseille |3   |Céline   |\n",
      "|null           |28  |Paris     |4   |David    |\n",
      "|null           |40  |Bordeaux  |5   |Emma     |\n",
      "|null           |22  |Nice      |6   |François |\n",
      "|null           |31  |Strasbourg|7   |Gabrielle|\n",
      "|null           |27  |Lille     |8   |Hugo     |\n",
      "|null           |29  |Nantes    |9   |Inès     |\n",
      "+---------------+----+----------+----+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_persons = spark.read.json('data/persons.json')\n",
    "df_persons.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7e246c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----+----------+----+---------+\n",
      "|_corrupt_record|age |city      |id  |name     |\n",
      "+---------------+----+----------+----+---------+\n",
      "|[              |null|null      |null|null     |\n",
      "|null           |30  |Paris     |1   |Alice    |\n",
      "|null           |25  |Lyon      |2   |Bob      |\n",
      "|null           |35  |Marseille |3   |Céline   |\n",
      "|null           |28  |Paris     |4   |David    |\n",
      "|null           |40  |Bordeaux  |5   |Emma     |\n",
      "|null           |22  |Nice      |6   |François |\n",
      "|null           |31  |Strasbourg|7   |Gabrielle|\n",
      "|null           |27  |Lille     |8   |Hugo     |\n",
      "|null           |29  |Nantes    |9   |Inès     |\n",
      "+---------------+----+----------+----+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+---------------+---+----------+---+---------+\n",
      "|_corrupt_record|age|city      |id |name     |\n",
      "+---------------+---+----------+---+---------+\n",
      "|null           |30 |Paris     |1  |Alice    |\n",
      "|null           |25 |Lyon      |2  |Bob      |\n",
      "|null           |35 |Marseille |3  |Céline   |\n",
      "|null           |28 |Paris     |4  |David    |\n",
      "|null           |40 |Bordeaux  |5  |Emma     |\n",
      "|null           |22 |Nice      |6  |François |\n",
      "|null           |31 |Strasbourg|7  |Gabrielle|\n",
      "|null           |27 |Lille     |8  |Hugo     |\n",
      "|null           |29 |Nantes    |9  |Inès     |\n",
      "|null           |33 |Toulouse  |10 |Julien   |\n",
      "+---------------+---+----------+---+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# oeioke_no_corrupt_data\n",
    "df_persons = spark.read.json('data/persons.json')\n",
    "df_persons.show(10, truncate=False)\n",
    "\n",
    "df_persons = df_persons.filter(df_persons['_corrupt_record'].isNull())\n",
    "df_persons.dropna(how= 'all')\n",
    "df_persons.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc6b6521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---+---------+\n",
      "|age|city      |id |name     |\n",
      "+---+----------+---+---------+\n",
      "|30 |Paris     |1  |Alice    |\n",
      "|25 |Lyon      |2  |Bob      |\n",
      "|35 |Marseille |3  |Céline   |\n",
      "|28 |Paris     |4  |David    |\n",
      "|40 |Bordeaux  |5  |Emma     |\n",
      "|22 |Nice      |6  |François |\n",
      "|31 |Strasbourg|7  |Gabrielle|\n",
      "|27 |Lille     |8  |Hugo     |\n",
      "|29 |Nantes    |9  |Inès     |\n",
      "|33 |Toulouse  |10 |Julien   |\n",
      "+---+----------+---+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_persons = spark.read.option('multiline', True).json('data/persons.json')\n",
    "df_persons.show(10, truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1ba24167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---+---------+\n",
      "|age|city      |id |name     |\n",
      "+---+----------+---+---------+\n",
      "|30 |Paris     |1  |Alice    |\n",
      "|25 |Lyon      |2  |Bob      |\n",
      "|35 |Marseille |3  |Céline   |\n",
      "|28 |Paris     |4  |David    |\n",
      "|40 |Bordeaux  |5  |Emma     |\n",
      "|22 |Nice      |6  |François |\n",
      "|31 |Strasbourg|7  |Gabrielle|\n",
      "|27 |Lille     |8  |Hugo     |\n",
      "|29 |Nantes    |9  |Inès     |\n",
      "|33 |Toulouse  |10 |Julien   |\n",
      "+---+----------+---+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_persons = spark.read.json('data/persons.json')\n",
    "\n",
    "df_persons=df_persons.drop('_corrupt_record')\n",
    "df_persons=df_persons.dropna(how= 'all')\n",
    "df_persons.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8cc02e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+\n",
      "|id |name     |age|\n",
      "+---+---------+---+\n",
      "|1  |Alice    |30 |\n",
      "|2  |Bob      |25 |\n",
      "|3  |Céline   |35 |\n",
      "|4  |David    |28 |\n",
      "|5  |Emma     |40 |\n",
      "|6  |François |22 |\n",
      "|7  |Gabrielle|31 |\n",
      "|8  |Hugo     |27 |\n",
      "|9  |Inès     |29 |\n",
      "|10 |Julien   |33 |\n",
      "+---+---------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_persons.select('id', 'name', 'age').show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a27a59d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+\n",
      "| id|     name|age|\n",
      "+---+---------+---+\n",
      "|  1|    Alice| 30|\n",
      "|  2|      Bob| 25|\n",
      "|  3|   Céline| 35|\n",
      "|  4|    David| 28|\n",
      "|  5|     Emma| 40|\n",
      "|  6| François| 22|\n",
      "|  7|Gabrielle| 31|\n",
      "|  8|     Hugo| 27|\n",
      "|  9|     Inès| 29|\n",
      "| 10|   Julien| 33|\n",
      "+---+---------+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_persons.select('id', 'name', 'age').show(10, truncate = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2601f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_persons.select(F.max('age')).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17b84a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_persons.groupBy('age').count().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Spark)",
   "language": "python",
   "name": "spark-env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
