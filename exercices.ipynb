{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd2dc6ed",
   "metadata": {},
   "source": [
    "### Exercices\n",
    "1) Envoyer le fichier persons.json dans un topic Kakfka\n",
    "2) Lire le topic en streaming avec Spark\n",
    "3) Stocker les données en DB dans une table raw_data\n",
    "4) Lecture en batch des données depuis la table raw_data\n",
    "5) Parser les données JSON\n",
    "6) Stocker les données nettoyées dans une table persons_data\n",
    "7) Filtrer les personnes de plus de 30 ans\n",
    "8) Compter le nombre de personne par ville\n",
    "9) Ajouter une colonne age_group( yyoung -18, adult 18-64, senior 65+) et les afficher\n",
    "10) Stocker les données nettoyés dans une table persons\n",
    "11) Grouper les donnés par villes et age_group et les afficher le total de chaque groupe\n",
    "12) Stocker les données agrégées dans une table persons_aggregated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f17e14",
   "metadata": {},
   "source": [
    "1) Envoyer le fichier persons.json dans un topic Kakfka\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bed4f69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from kafka.admin import KafkaAdminClient, NewTopic\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "topic = 'persons-exercice'\n",
    "kafka_server = 'kafka:9092'\n",
    "\n",
    "admin = KafkaAdminClient(bootstrap_servers=kafka_server)\n",
    "topic_list = admin.list_topics()\n",
    "if topic not in topic_list:\n",
    "    new_topic = NewTopic(name=topic, num_partitions=1, replication_factor=1)\n",
    "    admin.create_topics(new_topics=[new_topic])\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers=[kafka_server], value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n",
    "\n",
    "with open('data/persons.json', 'r') as file:\n",
    "    persons = json.load(file)\n",
    "    for person in persons:\n",
    "        producer.send(topic, value=person)\n",
    "\n",
    "    producer.flush()\n",
    "\n",
    "producer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d8d6d0",
   "metadata": {},
   "source": [
    "2) Lire le topic en streaming avec Spark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7beff9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/10/08 07:08:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/10/08 07:08:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/10/08 07:08:19 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-98a523ee-cee3-4646-94f9-e7bf64ce9b90. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/10/08 07:08:19 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----+--------------------+----------------+---------+------+--------------------+-------------+\n",
      "| key|               value|           topic|partition|offset|           timestamp|timestampType|\n",
      "+----+--------------------+----------------+---------+------+--------------------+-------------+\n",
      "|null|[7B 22 69 64 22 3...|persons-exercice|        0|     0|2025-10-08 07:07:...|            0|\n",
      "|null|[7B 22 69 64 22 3...|persons-exercice|        0|     1|2025-10-08 07:07:...|            0|\n",
      "|null|[7B 22 69 64 22 3...|persons-exercice|        0|     2|2025-10-08 07:07:...|            0|\n",
      "|null|[7B 22 69 64 22 3...|persons-exercice|        0|     3|2025-10-08 07:07:...|            0|\n",
      "|null|[7B 22 69 64 22 3...|persons-exercice|        0|     4|2025-10-08 07:07:...|            0|\n",
      "|null|[7B 22 69 64 22 3...|persons-exercice|        0|     5|2025-10-08 07:07:...|            0|\n",
      "|null|[7B 22 69 64 22 3...|persons-exercice|        0|     6|2025-10-08 07:07:...|            0|\n",
      "|null|[7B 22 69 64 22 3...|persons-exercice|        0|     7|2025-10-08 07:07:...|            0|\n",
      "|null|[7B 22 69 64 22 3...|persons-exercice|        0|     8|2025-10-08 07:07:...|            0|\n",
      "|null|[7B 22 69 64 22 3...|persons-exercice|        0|     9|2025-10-08 07:07:...|            0|\n",
      "|null|[7B 22 69 64 22 3...|persons-exercice|        0|    10|2025-10-08 07:07:...|            0|\n",
      "|null|[7B 22 69 64 22 3...|persons-exercice|        0|    11|2025-10-08 07:07:...|            0|\n",
      "|null|[7B 22 69 64 22 3...|persons-exercice|        0|    12|2025-10-08 07:07:...|            0|\n",
      "|null|[7B 22 69 64 22 3...|persons-exercice|        0|    13|2025-10-08 07:07:...|            0|\n",
      "|null|[7B 22 69 64 22 3...|persons-exercice|        0|    14|2025-10-08 07:07:...|            0|\n",
      "|null|[7B 22 69 64 22 3...|persons-exercice|        0|    15|2025-10-08 07:07:...|            0|\n",
      "|null|[7B 22 69 64 22 3...|persons-exercice|        0|    16|2025-10-08 07:07:...|            0|\n",
      "|null|[7B 22 69 64 22 3...|persons-exercice|        0|    17|2025-10-08 07:07:...|            0|\n",
      "|null|[7B 22 69 64 22 3...|persons-exercice|        0|    18|2025-10-08 07:07:...|            0|\n",
      "|null|[7B 22 69 64 22 3...|persons-exercice|        0|    19|2025-10-08 07:07:...|            0|\n",
      "+----+--------------------+----------------+---------+------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "topic = 'persons-exercice'\n",
    "spark = SparkSession.builder.appName(\"SparkStreamingDemo\").getOrCreate()\n",
    "df_stream = spark.readStream.format(\"kafka\") \\\n",
    "    .option('kafka.bootstrap.servers', kafka_server) \\\n",
    "    .option('subscribe', topic) \\\n",
    "    .option('startingOffsets', 'earliest') \\\n",
    "    .load()\n",
    "\n",
    "query = df_stream.writeStream.format(\"console\").start()\n",
    "query.awaitTermination(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ddd668",
   "metadata": {},
   "source": [
    "3) Stocker les données en DB dans une table raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46735672",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/08 07:10:34 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "25/10/08 07:10:34 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-55633f0f-b28f-4a99-939a-9f1d6d7d3ff8. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/10/08 07:10:34 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+-----------+-----------+\n",
      "|id |name     |age|city       |age_group  |\n",
      "+---+---------+---+-----------+-----------+\n",
      "|1  |Alice    |30 |Paris      |adult 18-64|\n",
      "|2  |Bob      |25 |Lyon       |adult 18-64|\n",
      "|3  |Céline   |35 |Marseille  |adult 18-64|\n",
      "|4  |David    |28 |Paris      |adult 18-64|\n",
      "|5  |Emma     |40 |Bordeaux   |adult 18-64|\n",
      "|6  |François |22 |Nice       |adult 18-64|\n",
      "|7  |Gabrielle|31 |Strasbourg |adult 18-64|\n",
      "|8  |Hugo     |27 |Lille      |adult 18-64|\n",
      "|9  |Inès     |29 |Nantes     |adult 18-64|\n",
      "|10 |Julien   |33 |Toulouse   |adult 18-64|\n",
      "|11 |Karim    |41 |Montpellier|adult 18-64|\n",
      "|12 |Laura    |36 |Paris      |adult 18-64|\n",
      "|13 |Mathieu  |24 |Lyon       |adult 18-64|\n",
      "|14 |Nina     |26 |Marseille  |adult 18-64|\n",
      "|15 |Olivier  |39 |Bordeaux   |adult 18-64|\n",
      "|16 |Pauline  |34 |Nice       |adult 18-64|\n",
      "|17 |Quentin  |23 |Strasbourg |adult 18-64|\n",
      "|18 |Rania    |32 |Lille      |adult 18-64|\n",
      "|19 |Sophie   |38 |Nantes     |adult 18-64|\n",
      "|20 |Thomas   |27 |Toulouse   |adult 18-64|\n",
      "+---+---------+---+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+---+-----------+-----------+\n",
      "|id |name     |age|city       |age_group  |\n",
      "+---+---------+---+-----------+-----------+\n",
      "|1  |Alice    |30 |Paris      |adult 18-64|\n",
      "|2  |Bob      |25 |Lyon       |adult 18-64|\n",
      "|3  |Céline   |35 |Marseille  |adult 18-64|\n",
      "|4  |David    |28 |Paris      |adult 18-64|\n",
      "|5  |Emma     |40 |Bordeaux   |adult 18-64|\n",
      "|6  |François |22 |Nice       |adult 18-64|\n",
      "|7  |Gabrielle|31 |Strasbourg |adult 18-64|\n",
      "|8  |Hugo     |27 |Lille      |adult 18-64|\n",
      "|9  |Inès     |29 |Nantes     |adult 18-64|\n",
      "|10 |Julien   |33 |Toulouse   |adult 18-64|\n",
      "|11 |Karim    |41 |Montpellier|adult 18-64|\n",
      "|12 |Laura    |36 |Paris      |adult 18-64|\n",
      "|13 |Mathieu  |24 |Lyon       |adult 18-64|\n",
      "|14 |Nina     |26 |Marseille  |adult 18-64|\n",
      "|15 |Olivier  |39 |Bordeaux   |adult 18-64|\n",
      "|16 |Pauline  |34 |Nice       |adult 18-64|\n",
      "|17 |Quentin  |23 |Strasbourg |adult 18-64|\n",
      "|18 |Rania    |32 |Lille      |adult 18-64|\n",
      "|19 |Sophie   |38 |Nantes     |adult 18-64|\n",
      "|20 |Thomas   |27 |Toulouse   |adult 18-64|\n",
      "+---+---------+---+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+---+---------+---+-----------+-----------+\n",
      "|id |name     |age|city       |age_group  |\n",
      "+---+---------+---+-----------+-----------+\n",
      "|1  |Alice    |30 |Paris      |adult 18-64|\n",
      "|2  |Bob      |25 |Lyon       |adult 18-64|\n",
      "|3  |Céline   |35 |Marseille  |adult 18-64|\n",
      "|4  |David    |28 |Paris      |adult 18-64|\n",
      "|5  |Emma     |40 |Bordeaux   |adult 18-64|\n",
      "|6  |François |22 |Nice       |adult 18-64|\n",
      "|7  |Gabrielle|31 |Strasbourg |adult 18-64|\n",
      "|8  |Hugo     |27 |Lille      |adult 18-64|\n",
      "|9  |Inès     |29 |Nantes     |adult 18-64|\n",
      "|10 |Julien   |33 |Toulouse   |adult 18-64|\n",
      "|11 |Karim    |41 |Montpellier|adult 18-64|\n",
      "|12 |Laura    |36 |Paris      |adult 18-64|\n",
      "|13 |Mathieu  |24 |Lyon       |adult 18-64|\n",
      "|14 |Nina     |26 |Marseille  |adult 18-64|\n",
      "|15 |Olivier  |39 |Bordeaux   |adult 18-64|\n",
      "|16 |Pauline  |34 |Nice       |adult 18-64|\n",
      "|17 |Quentin  |23 |Strasbourg |adult 18-64|\n",
      "|18 |Rania    |32 |Lille      |adult 18-64|\n",
      "|19 |Sophie   |38 |Nantes     |adult 18-64|\n",
      "|20 |Thomas   |27 |Toulouse   |adult 18-64|\n",
      "+---+---------+---+-----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkExercice\").getOrCreate()\n",
    "df_stream = spark.readStream.format(\"kafka\") \\\n",
    "    .option('kafka.bootstrap.servers', kafka_server) \\\n",
    "    .option('subscribe', topic) \\\n",
    "    .option('startingOffsets', 'earliest') \\\n",
    "    .load()\n",
    "\n",
    "def process_batch(batch_df, batch_id):\n",
    "    batch_df.write.format(\"jdbc\") \\\n",
    "        .option(\"url\", \"jdbc:postgresql://postgres:5432/events\") \\\n",
    "        .option(\"dbtable\", \"public.raw_data\") \\\n",
    "        .option(\"user\", \"app\") \\\n",
    "        .option(\"password\", \"1234\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "\n",
    "query = df_stream.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .start()\n",
    "query.awaitTermination(20)\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593b4184",
   "metadata": {},
   "source": [
    "4) Lecture en batch des données depuis la table raw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdf0d934",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/08 07:14:16 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------+---------+------+-----------------------+-------------+\n",
      "|key |value                                                                                                                                                                                            |topic           |partition|offset|timestamp              |timestampType|\n",
      "+----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------+---------+------+-----------------------+-------------+\n",
      "|null|[7B 22 69 64 22 3A 20 31 2C 20 22 6E 61 6D 65 22 3A 20 22 41 6C 69 63 65 22 2C 20 22 61 67 65 22 3A 20 33 30 2C 20 22 63 69 74 79 22 3A 20 22 50 61 72 69 73 22 7D]                              |persons-exercice|0        |0     |2025-10-08 07:07:33.131|0            |\n",
      "|null|[7B 22 69 64 22 3A 20 32 2C 20 22 6E 61 6D 65 22 3A 20 22 42 6F 62 22 2C 20 22 61 67 65 22 3A 20 32 35 2C 20 22 63 69 74 79 22 3A 20 22 4C 79 6F 6E 22 7D]                                       |persons-exercice|0        |1     |2025-10-08 07:07:33.132|0            |\n",
      "|null|[7B 22 69 64 22 3A 20 33 2C 20 22 6E 61 6D 65 22 3A 20 22 43 5C 75 30 30 65 39 6C 69 6E 65 22 2C 20 22 61 67 65 22 3A 20 33 35 2C 20 22 63 69 74 79 22 3A 20 22 4D 61 72 73 65 69 6C 6C 65 22 7D]|persons-exercice|0        |2     |2025-10-08 07:07:33.132|0            |\n",
      "|null|[7B 22 69 64 22 3A 20 34 2C 20 22 6E 61 6D 65 22 3A 20 22 44 61 76 69 64 22 2C 20 22 61 67 65 22 3A 20 32 38 2C 20 22 63 69 74 79 22 3A 20 22 50 61 72 69 73 22 7D]                              |persons-exercice|0        |3     |2025-10-08 07:07:33.132|0            |\n",
      "|null|[7B 22 69 64 22 3A 20 35 2C 20 22 6E 61 6D 65 22 3A 20 22 45 6D 6D 61 22 2C 20 22 61 67 65 22 3A 20 34 30 2C 20 22 63 69 74 79 22 3A 20 22 42 6F 72 64 65 61 75 78 22 7D]                        |persons-exercice|0        |4     |2025-10-08 07:07:33.132|0            |\n",
      "|null|[7B 22 69 64 22 3A 20 36 2C 20 22 6E 61 6D 65 22 3A 20 22 46 72 61 6E 5C 75 30 30 65 37 6F 69 73 22 2C 20 22 61 67 65 22 3A 20 32 32 2C 20 22 63 69 74 79 22 3A 20 22 4E 69 63 65 22 7D]         |persons-exercice|0        |5     |2025-10-08 07:07:33.132|0            |\n",
      "|null|[7B 22 69 64 22 3A 20 37 2C 20 22 6E 61 6D 65 22 3A 20 22 47 61 62 72 69 65 6C 6C 65 22 2C 20 22 61 67 65 22 3A 20 33 31 2C 20 22 63 69 74 79 22 3A 20 22 53 74 72 61 73 62 6F 75 72 67 22 7D]   |persons-exercice|0        |6     |2025-10-08 07:07:33.132|0            |\n",
      "|null|[7B 22 69 64 22 3A 20 38 2C 20 22 6E 61 6D 65 22 3A 20 22 48 75 67 6F 22 2C 20 22 61 67 65 22 3A 20 32 37 2C 20 22 63 69 74 79 22 3A 20 22 4C 69 6C 6C 65 22 7D]                                 |persons-exercice|0        |7     |2025-10-08 07:07:33.132|0            |\n",
      "|null|[7B 22 69 64 22 3A 20 39 2C 20 22 6E 61 6D 65 22 3A 20 22 49 6E 5C 75 30 30 65 38 73 22 2C 20 22 61 67 65 22 3A 20 32 39 2C 20 22 63 69 74 79 22 3A 20 22 4E 61 6E 74 65 73 22 7D]               |persons-exercice|0        |8     |2025-10-08 07:07:33.132|0            |\n",
      "|null|[7B 22 69 64 22 3A 20 31 30 2C 20 22 6E 61 6D 65 22 3A 20 22 4A 75 6C 69 65 6E 22 2C 20 22 61 67 65 22 3A 20 33 33 2C 20 22 63 69 74 79 22 3A 20 22 54 6F 75 6C 6F 75 73 65 22 7D]               |persons-exercice|0        |9     |2025-10-08 07:07:33.133|0            |\n",
      "|null|[7B 22 69 64 22 3A 20 31 31 2C 20 22 6E 61 6D 65 22 3A 20 22 4B 61 72 69 6D 22 2C 20 22 61 67 65 22 3A 20 34 31 2C 20 22 63 69 74 79 22 3A 20 22 4D 6F 6E 74 70 65 6C 6C 69 65 72 22 7D]         |persons-exercice|0        |10    |2025-10-08 07:07:33.133|0            |\n",
      "|null|[7B 22 69 64 22 3A 20 31 32 2C 20 22 6E 61 6D 65 22 3A 20 22 4C 61 75 72 61 22 2C 20 22 61 67 65 22 3A 20 33 36 2C 20 22 63 69 74 79 22 3A 20 22 50 61 72 69 73 22 7D]                           |persons-exercice|0        |11    |2025-10-08 07:07:33.133|0            |\n",
      "|null|[7B 22 69 64 22 3A 20 31 33 2C 20 22 6E 61 6D 65 22 3A 20 22 4D 61 74 68 69 65 75 22 2C 20 22 61 67 65 22 3A 20 32 34 2C 20 22 63 69 74 79 22 3A 20 22 4C 79 6F 6E 22 7D]                        |persons-exercice|0        |12    |2025-10-08 07:07:33.133|0            |\n",
      "|null|[7B 22 69 64 22 3A 20 31 34 2C 20 22 6E 61 6D 65 22 3A 20 22 4E 69 6E 61 22 2C 20 22 61 67 65 22 3A 20 32 36 2C 20 22 63 69 74 79 22 3A 20 22 4D 61 72 73 65 69 6C 6C 65 22 7D]                  |persons-exercice|0        |13    |2025-10-08 07:07:33.133|0            |\n",
      "|null|[7B 22 69 64 22 3A 20 31 35 2C 20 22 6E 61 6D 65 22 3A 20 22 4F 6C 69 76 69 65 72 22 2C 20 22 61 67 65 22 3A 20 33 39 2C 20 22 63 69 74 79 22 3A 20 22 42 6F 72 64 65 61 75 78 22 7D]            |persons-exercice|0        |14    |2025-10-08 07:07:33.133|0            |\n",
      "|null|[7B 22 69 64 22 3A 20 31 36 2C 20 22 6E 61 6D 65 22 3A 20 22 50 61 75 6C 69 6E 65 22 2C 20 22 61 67 65 22 3A 20 33 34 2C 20 22 63 69 74 79 22 3A 20 22 4E 69 63 65 22 7D]                        |persons-exercice|0        |15    |2025-10-08 07:07:33.133|0            |\n",
      "|null|[7B 22 69 64 22 3A 20 31 37 2C 20 22 6E 61 6D 65 22 3A 20 22 51 75 65 6E 74 69 6E 22 2C 20 22 61 67 65 22 3A 20 32 33 2C 20 22 63 69 74 79 22 3A 20 22 53 74 72 61 73 62 6F 75 72 67 22 7D]      |persons-exercice|0        |16    |2025-10-08 07:07:33.133|0            |\n",
      "|null|[7B 22 69 64 22 3A 20 31 38 2C 20 22 6E 61 6D 65 22 3A 20 22 52 61 6E 69 61 22 2C 20 22 61 67 65 22 3A 20 33 32 2C 20 22 63 69 74 79 22 3A 20 22 4C 69 6C 6C 65 22 7D]                           |persons-exercice|0        |17    |2025-10-08 07:07:33.133|0            |\n",
      "|null|[7B 22 69 64 22 3A 20 31 39 2C 20 22 6E 61 6D 65 22 3A 20 22 53 6F 70 68 69 65 22 2C 20 22 61 67 65 22 3A 20 33 38 2C 20 22 63 69 74 79 22 3A 20 22 4E 61 6E 74 65 73 22 7D]                     |persons-exercice|0        |18    |2025-10-08 07:07:33.133|0            |\n",
      "|null|[7B 22 69 64 22 3A 20 32 30 2C 20 22 6E 61 6D 65 22 3A 20 22 54 68 6F 6D 61 73 22 2C 20 22 61 67 65 22 3A 20 32 37 2C 20 22 63 69 74 79 22 3A 20 22 54 6F 75 6C 6F 75 73 65 22 7D]               |persons-exercice|0        |19    |2025-10-08 07:07:33.133|0            |\n",
      "+----+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------+---------+------+-----------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ReadRawData\").getOrCreate()\n",
    "\n",
    "df_raw = spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres:5432/events\") \\\n",
    "    .option(\"dbtable\", \"public.raw_data\") \\\n",
    "    .option(\"user\", \"app\") \\\n",
    "    .option(\"password\", \"1234\") \\\n",
    "    .load()\n",
    "\n",
    "df_raw.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223457bf",
   "metadata": {},
   "source": [
    "5) Parser les données JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b9c3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/08 07:15:59 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SparkExercice\").getOrCreate()\n",
    "df_stream = spark.readStream.format(\"kafka\") \\\n",
    "    .option('kafka.bootstrap.servers', kafka_server) \\\n",
    "    .option('subscribe', topic) \\\n",
    "    .option('startingOffsets', 'earliest') \\\n",
    "    .load()\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"age\", IntegerType(), False),\n",
    "    StructField(\"city\", StringType(), False)\n",
    "])\n",
    "\n",
    "lines = df_stream.selectExpr(\"CAST(value AS STRING) as json_str\") \\\n",
    "    .select(from_json(col(\"json_str\"), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548728b7",
   "metadata": {},
   "source": [
    "6) Stocker les données nettoyées dans une table persons_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d03c3823",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/10/08 07:17:17 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-8779dac5-4ef2-452b-bf00-033aa79961b9. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "25/10/08 07:17:17 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def process_batch(batch_df, batch_id):\n",
    "    batch_df.write.format(\"jdbc\") \\\n",
    "        .option(\"url\", \"jdbc:postgresql://postgres:5432/events\") \\\n",
    "        .option(\"dbtable\", \"public.persons_data\") \\\n",
    "        .option(\"user\", \"app\") \\\n",
    "        .option(\"password\", \"1234\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "\n",
    "query = lines.writeStream \\\n",
    "    .foreachBatch(process_batch) \\\n",
    "    .start()\n",
    "query.awaitTermination(10)\n",
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443942bc",
   "metadata": {},
   "source": [
    "7) Filtrer les personnes de plus de 30 ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bd38b1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---+-----------+\n",
      "| id|     name|age|       city|\n",
      "+---+---------+---+-----------+\n",
      "|  3|   Céline| 35|  Marseille|\n",
      "|  5|     Emma| 40|   Bordeaux|\n",
      "|  7|Gabrielle| 31| Strasbourg|\n",
      "| 10|   Julien| 33|   Toulouse|\n",
      "| 11|    Karim| 41|Montpellier|\n",
      "| 12|    Laura| 36|      Paris|\n",
      "| 15|  Olivier| 39|   Bordeaux|\n",
      "| 16|  Pauline| 34|       Nice|\n",
      "| 18|    Rania| 32|      Lille|\n",
      "| 19|   Sophie| 38|     Nantes|\n",
      "| 23|  William| 37|       Lyon|\n",
      "| 24|   Xavier| 45|  Marseille|\n",
      "| 27|   Adrien| 33| Strasbourg|\n",
      "| 28| Brigitte| 42|      Lille|\n",
      "| 30| Delphine| 31|   Toulouse|\n",
      "| 31|     Éric| 40|Montpellier|\n",
      "| 33|  Georges| 36|       Lyon|\n",
      "| 36|    Julie| 34|       Nice|\n",
      "| 37|    Kevin| 39| Strasbourg|\n",
      "| 40|    Nadia| 37|   Toulouse|\n",
      "+---+---------+---+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_persons = spark.read.format('jdbc')\\\n",
    "        .option(\"url\", \"jdbc:postgresql://postgres:5432/events\") \\\n",
    "        .option(\"dbtable\", \"public.persons_data\") \\\n",
    "        .option(\"user\", \"app\") \\\n",
    "        .option(\"password\", \"1234\") \\\n",
    "        .load()\n",
    "\n",
    "df_persons.filter(df_persons.age >30).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f6c1f8",
   "metadata": {},
   "source": [
    "8) Compter le nombre de personne par ville"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d5a5b0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+\n",
      "|       city|count(name)|\n",
      "+-----------+-----------+\n",
      "|       Nice|          5|\n",
      "|Montpellier|          4|\n",
      "|      Lille|          5|\n",
      "|     Nantes|          5|\n",
      "|  Marseille|          5|\n",
      "|      Paris|          6|\n",
      "|       Lyon|          5|\n",
      "|   Bordeaux|          5|\n",
      "| Strasbourg|          5|\n",
      "|   Toulouse|          5|\n",
      "+-----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, sum\n",
    "\n",
    "df_persons.groupBy(\"city\").agg(count(\"name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbabc719",
   "metadata": {},
   "source": [
    "9) Ajouter une colonne age_group( young -18, adult 18-64, senior 65+) et les afficher\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a57e2050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, lit\n",
    "\n",
    "df=df_persons.withColumn(\"age_group\",\n",
    "                      when((df_persons.age < 18), lit(\"young -18\")).\n",
    "                      when((df_persons.age >= 18) & (\n",
    "                          df_persons.age < 65), lit(\"adult 18-64\")).otherwise(lit(\"senior 65+\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9143fb0",
   "metadata": {},
   "source": [
    "10) Stocker les données nettoyés dans une table persons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3206b06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, lit\n",
    "\n",
    "# Write to PostgreSQL\n",
    "df.write.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres:5432/events\") \\\n",
    "    .option(\"dbtable\", \"public.persons\") \\\n",
    "    .option(\"user\", \"app\") \\\n",
    "    .option(\"password\", \"1234\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366001c8",
   "metadata": {},
   "source": [
    "11) Grouper les donnés par villes et age_group et les afficher le total de chaque groupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "52623ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "df_grouped =df.groupBy(\"age_group\",\"city\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d7aabf",
   "metadata": {},
   "source": [
    "12) Stocker les données agrégées dans une table persons_aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "52bbe7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, lit\n",
    "\n",
    "# Write to PostgreSQL\n",
    "df_grouped.write.format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://postgres:5432/events\") \\\n",
    "    .option(\"dbtable\", \"public.persons_aggregated\") \\\n",
    "    .option(\"user\", \"app\") \\\n",
    "    .option(\"password\", \"1234\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Spark)",
   "language": "python",
   "name": "spark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
